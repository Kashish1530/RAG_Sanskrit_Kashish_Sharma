ğŸ“š Sanskrit Story Retrieval System (RAG)

ğŸŒŸ Overview

A hybrid Retrieval-Augmented Generation (RAG) system for Sanskrit stories, combining semantic embeddings and keyword matching. It retrieves contextually relevant answers from a curated corpus using FAISS and a quantized Qwen model, fully optimized for CPU deployment.

âš¡ Key Features

Hybrid Search: Dense embeddings + BM25 keyword matching

Offline & Efficient: Runs locally on CPU using quantized Qwen

Contextual Responses: LLM generates coherent answers from retrieved passages

Fast Retrieval: FAISS-powered vector search

RRF Fusion: Combines results from vector and keyword search for better accuracy

ğŸ›  Technical Components
Component	Details
Embeddings	sentence-transformers/all-MiniLM-L6-v2, 384-dim
Vector Store	FAISS, cosine similarity, CPU-optimized
Language Model	Qwen GGUF, llama-cpp-python, 2048-token context
Keyword Matcher	BM25 (k1=1.5, b=0.75)
Text Splitter	RecursiveCharacterTextSplitter, 500-char chunks, 100-char overlap

ğŸ“ Folder Structure

RAG_Sanskrit_Kashish/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ app.py           # Query interface & RAG execution
â”‚   â”œâ”€â”€ ingest.py        # Document ingestion & indexing
â”‚   â”œâ”€â”€ utils.py         # Helper functions
â”‚   â”œâ”€â”€ faiss_index/     # Stored vector index
â”‚   â”œâ”€â”€ qwen.gguf        # CPU-compatible LLM model
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ devbhakta.txt
â”‚   â”œâ”€â”€ ghantakarna.txt
â”‚   â”œâ”€â”€ kalidasa.txt
â”‚   â”œâ”€â”€ murkhabhriya.txt
â”‚   â””â”€â”€ sheetam.txt
â”‚
â”œâ”€â”€ venv/                # Python virtual environment
â”œâ”€â”€ README.md
â””â”€â”€ report/
    â””â”€â”€ Sanskrit_RAG_Report.pdf

ğŸ¯ Highlights

CPU-friendly: No GPU required, runs locally

Hybrid retrieval: Combines semantic understanding & exact keyword match

Context-aware answers: Preserves story context with chunked embeddings

Scalable: Easily add more Sanskrit stories without major changes
